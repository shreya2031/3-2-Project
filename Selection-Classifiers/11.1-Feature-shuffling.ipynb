{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection by Random Shuffling\n",
    "\n",
    "A popular method of feature selection consists in random shuffling the values of a specific variable and determining how that permutation affects the performance metric of the machine learning algorithm. In other words, the idea is to permute the values of each feature, one feature at the time, and measure how much the permutation (or shuffling of its values) decreases the accuracy, or the roc_auc, or the mse of the machine learning model (or any other performance metric!). If the variables are important, a random permutation of their values will decrease dramatically any of these metrics. Contrarily, the permutation or shuffling of values should have little to no effect on the model performance metric we are assessing.\n",
    "\n",
    "The procedure goes more or less like this:\n",
    "\n",
    "- Build a machine learning model and store its performance metric\n",
    "- Shuffle 1 feature, and make a new prediction using the previous model\n",
    "- Determine the performance of this prediction\n",
    "- Determine the change in the performance of the prediction with the shuffled feature vs the original one\n",
    "- Repeat for each feature\n",
    "\n",
    "To select features, we choose those that induced a decrease in model performance, beyond an arbitrarily set threshold.\n",
    "\n",
    "I will demonstrate how to select features based on random shuffling using on a regression and classification problem. \n",
    "\n",
    "**Note** For the demonstration, I will continue to use Random Forests, but this selection procedure can be used with machine learning algorithm. In fact, the importance of the features are determined specifically for the algorithm used. Therefore, different algorithms may return different subsets of important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 36)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('C:/Users/RAJENDRA REDDY/Downloads/Genre1.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chroma_stft_min</th>\n",
       "      <th>chroma_stft_max</th>\n",
       "      <th>chroma_cqt_min</th>\n",
       "      <th>chroma_cqt_max</th>\n",
       "      <th>chroma_cens_min</th>\n",
       "      <th>chroma_cens_max</th>\n",
       "      <th>melspectogram_min</th>\n",
       "      <th>melspectogram_max</th>\n",
       "      <th>mfcc_min</th>\n",
       "      <th>mfcc_max</th>\n",
       "      <th>...</th>\n",
       "      <th>zero_crossing_rate_min</th>\n",
       "      <th>zero_crossing_rate_max</th>\n",
       "      <th>tempogram_min</th>\n",
       "      <th>tempogram_max</th>\n",
       "      <th>delta_mfcc_min</th>\n",
       "      <th>delta_mfcc_max</th>\n",
       "      <th>mel_to_stft_min</th>\n",
       "      <th>mel_to_stft_max</th>\n",
       "      <th>class</th>\n",
       "      <th>song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001296</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033154</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>0.739581</td>\n",
       "      <td>8.890000e-06</td>\n",
       "      <td>6547.40700</td>\n",
       "      <td>-162.60739</td>\n",
       "      <td>148.07231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020020</td>\n",
       "      <td>0.305176</td>\n",
       "      <td>-2.850000e-16</td>\n",
       "      <td>1</td>\n",
       "      <td>-27.087835</td>\n",
       "      <td>25.198893</td>\n",
       "      <td>0</td>\n",
       "      <td>18.772789</td>\n",
       "      <td>1</td>\n",
       "      <td>Aa To Sahii (sahi)_shortened.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062056</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020606</td>\n",
       "      <td>0.682328</td>\n",
       "      <td>1.990000e-09</td>\n",
       "      <td>3179.20950</td>\n",
       "      <td>-243.84023</td>\n",
       "      <td>156.03381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.543457</td>\n",
       "      <td>-2.850000e-16</td>\n",
       "      <td>1</td>\n",
       "      <td>-24.831850</td>\n",
       "      <td>26.813145</td>\n",
       "      <td>0</td>\n",
       "      <td>14.955276</td>\n",
       "      <td>1</td>\n",
       "      <td>Aadat (23)_shortened.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003432</td>\n",
       "      <td>1</td>\n",
       "      <td>0.056286</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025010</td>\n",
       "      <td>0.674345</td>\n",
       "      <td>1.470000e-06</td>\n",
       "      <td>367.87683</td>\n",
       "      <td>-197.41306</td>\n",
       "      <td>134.92323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.480957</td>\n",
       "      <td>-3.320000e-16</td>\n",
       "      <td>1</td>\n",
       "      <td>-14.765142</td>\n",
       "      <td>14.908866</td>\n",
       "      <td>0</td>\n",
       "      <td>9.169767</td>\n",
       "      <td>1</td>\n",
       "      <td>Aag Chahat Ki Lag Jayegi (1)_shortened.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000696</td>\n",
       "      <td>1</td>\n",
       "      <td>0.049335</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.777123</td>\n",
       "      <td>8.430000e-07</td>\n",
       "      <td>5928.97400</td>\n",
       "      <td>-204.65260</td>\n",
       "      <td>162.19836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.195801</td>\n",
       "      <td>-2.440000e-16</td>\n",
       "      <td>1</td>\n",
       "      <td>-29.716740</td>\n",
       "      <td>21.724106</td>\n",
       "      <td>0</td>\n",
       "      <td>17.889960</td>\n",
       "      <td>1</td>\n",
       "      <td>Aahista Aahista (16)_shortened.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000197</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782509</td>\n",
       "      <td>2.600000e-11</td>\n",
       "      <td>722.85565</td>\n",
       "      <td>-351.27094</td>\n",
       "      <td>223.67530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>0.239258</td>\n",
       "      <td>-3.610000e-16</td>\n",
       "      <td>1</td>\n",
       "      <td>-22.297218</td>\n",
       "      <td>16.177706</td>\n",
       "      <td>0</td>\n",
       "      <td>10.572831</td>\n",
       "      <td>1</td>\n",
       "      <td>Aaiye Meharban (23)_shortened.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   chroma_stft_min  chroma_stft_max  chroma_cqt_min  chroma_cqt_max  \\\n",
       "0         0.001296                1        0.033154               1   \n",
       "1         0.002739                1        0.062056               1   \n",
       "2         0.003432                1        0.056286               1   \n",
       "3         0.000696                1        0.049335               1   \n",
       "4         0.000197                1        0.026210               1   \n",
       "\n",
       "   chroma_cens_min  chroma_cens_max  melspectogram_min  melspectogram_max  \\\n",
       "0         0.003514         0.739581       8.890000e-06         6547.40700   \n",
       "1         0.020606         0.682328       1.990000e-09         3179.20950   \n",
       "2         0.025010         0.674345       1.470000e-06          367.87683   \n",
       "3         0.000000         0.777123       8.430000e-07         5928.97400   \n",
       "4         0.000000         0.782509       2.600000e-11          722.85565   \n",
       "\n",
       "    mfcc_min   mfcc_max  ...  zero_crossing_rate_min  zero_crossing_rate_max  \\\n",
       "0 -162.60739  148.07231  ...                0.020020                0.305176   \n",
       "1 -243.84023  156.03381  ...                0.008301                0.543457   \n",
       "2 -197.41306  134.92323  ...                0.054688                0.480957   \n",
       "3 -204.65260  162.19836  ...                0.004883                0.195801   \n",
       "4 -351.27094  223.67530  ...                0.024902                0.239258   \n",
       "\n",
       "   tempogram_min  tempogram_max  delta_mfcc_min  delta_mfcc_max  \\\n",
       "0  -2.850000e-16              1      -27.087835       25.198893   \n",
       "1  -2.850000e-16              1      -24.831850       26.813145   \n",
       "2  -3.320000e-16              1      -14.765142       14.908866   \n",
       "3  -2.440000e-16              1      -29.716740       21.724106   \n",
       "4  -3.610000e-16              1      -22.297218       16.177706   \n",
       "\n",
       "   mel_to_stft_min  mel_to_stft_max  class  \\\n",
       "0                0        18.772789      1   \n",
       "1                0        14.955276      1   \n",
       "2                0         9.169767      1   \n",
       "3                0        17.889960      1   \n",
       "4                0        10.572831      1   \n",
       "\n",
       "                                         song  \n",
       "0            Aa To Sahii (sahi)_shortened.wav  \n",
       "1                    Aadat (23)_shortened.wav  \n",
       "2  Aag Chahat Ki Lag Jayegi (1)_shortened.wav  \n",
       "3          Aahista Aahista (16)_shortened.wav  \n",
       "4           Aaiye Meharban (23)_shortened.wav  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((140, 34), (60, 34))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "feature_cols = ['chroma_stft_min', 'chroma_stft_max', 'chroma_cqt_min',\n",
    "       'chroma_cqt_max', 'chroma_cens_min', 'chroma_cens_max',\n",
    "       'melspectogram_min', 'melspectogram_max', 'mfcc_min', 'mfcc_max',\n",
    "       'rms_min', 'rms_max', 'spectral_centroid_min', 'spectral_centroid_max',\n",
    "       'spectral_bandwidth_min', 'spectral_bandwidth_max',\n",
    "       'spectral_contrast_min', 'spectral_contrast_max',\n",
    "       'spectral_flatness_min', 'spectral_flatness_max',\n",
    "       'spectral_rolloff_min', 'spectral_rolloff_max', 'poly_features_min',\n",
    "       'poly_features_max', 'tonnetz_min', 'tonnetz_max',\n",
    "       'zero_crossing_rate_min', 'zero_crossing_rate_max', 'tempogram_min',\n",
    "       'tempogram_max', 'delta_mfcc_min', 'delta_mfcc_max', 'mel_to_stft_min',\n",
    "       'mel_to_stft_max']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[feature_cols],data['class'],test_size=0.3,random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this method, it is necessary to reset the indeces of the returned \n",
    "# datasets\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ML algo with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc score:  1.0\n",
      "test auc score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# The first step to determine feature importance by feature shuffling\n",
    "# is to build the machine learning model for which we want to \n",
    "# select features\n",
    "\n",
    "# In this case, I will build Random Forests, but remember that \n",
    "# you can use this procedure with any other machine learning algorithm\n",
    "\n",
    "# I build few and shallow trees to avoid overfitting\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50, max_depth=2, random_state=2909, n_jobs=4)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# print roc-auc in train and testing sets\n",
    "print('train auc score: ',\n",
    "      accuracy_score(y_train, (rf.predict_proba(X_train.fillna(0)))))\n",
    "print('test auc score: ',\n",
    "      accuracy_score(y_test, (rf.predict_proba(X_test.fillna(0)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle features and asses performance drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell, I will shuffle one by one, each feature of the dataset\n",
    "\n",
    "# then I use the dataset with the shuffled variable to make predictions\n",
    "# with the random forests I trained in the previous cell\n",
    "\n",
    "# overall train roc-auc: using all the features\n",
    "train_roc = roc_auc_score(y_train, (rf.predict_proba(X_train)),multi_class=\"ovr\")\n",
    "\n",
    "# list to capture the performance shift\n",
    "performance_shift = []\n",
    "\n",
    "# selection  logic\n",
    "for feature in X_train.columns:\n",
    "\n",
    "    X_train_c = X_train.copy()\n",
    "\n",
    "    # shuffle individual feature\n",
    "    X_train_c[feature] = X_train_c[feature].sample(\n",
    "        frac=1, random_state=10).reset_index(drop=True)\n",
    "\n",
    "    # make prediction with shuffled feature and calculate roc-auc\n",
    "    shuff_roc = roc_auc_score(y_train, rf.predict_proba(X_train_c),multi_class=\"ovr\")\n",
    "    \n",
    "    drift = train_roc - shuff_roc\n",
    "\n",
    "    # save the drop in roc-auc\n",
    "    performance_shift.append(drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623,\n",
       " 0.0003287388914665623]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# le't have a look at our list of performances\n",
    "performance_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chroma_stft_min    0.000329\n",
       "chroma_stft_max    0.000329\n",
       "chroma_cqt_min     0.000329\n",
       "chroma_cqt_max     0.000329\n",
       "chroma_cens_min    0.000329\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now I will transform the list into a pandas Series\n",
    "# for easy manipulation\n",
    "\n",
    "feature_importance = pd.Series(performance_shift)\n",
    "\n",
    "# add variable names in the index\n",
    "feature_importance.index = X_train.columns\n",
    "\n",
    "feature_importance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mel_to_stft_max           0.000329\n",
       "mfcc_min                  0.000329\n",
       "spectral_bandwidth_min    0.000329\n",
       "spectral_centroid_max     0.000329\n",
       "spectral_centroid_min     0.000329\n",
       "rms_max                   0.000329\n",
       "rms_min                   0.000329\n",
       "mfcc_max                  0.000329\n",
       "melspectogram_max         0.000329\n",
       "mel_to_stft_min           0.000329\n",
       "melspectogram_min         0.000329\n",
       "chroma_cens_max           0.000329\n",
       "chroma_cens_min           0.000329\n",
       "chroma_cqt_max            0.000329\n",
       "chroma_cqt_min            0.000329\n",
       "chroma_stft_max           0.000329\n",
       "spectral_bandwidth_max    0.000329\n",
       "spectral_contrast_min     0.000329\n",
       "spectral_contrast_max     0.000329\n",
       "spectral_flatness_min     0.000329\n",
       "spectral_flatness_max     0.000329\n",
       "spectral_rolloff_min      0.000329\n",
       "spectral_rolloff_max      0.000329\n",
       "poly_features_min         0.000329\n",
       "poly_features_max         0.000329\n",
       "tonnetz_min               0.000329\n",
       "tonnetz_max               0.000329\n",
       "zero_crossing_rate_min    0.000329\n",
       "zero_crossing_rate_max    0.000329\n",
       "tempogram_min             0.000329\n",
       "tempogram_max             0.000329\n",
       "delta_mfcc_min            0.000329\n",
       "delta_mfcc_max            0.000329\n",
       "chroma_stft_min           0.000329\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now I will sort the dataframe according to the drop in performance\n",
    "# caused by feature shuffling\n",
    "\n",
    "feature_importance.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mel_to_stft_max           0.000329\n",
       "mfcc_min                  0.000329\n",
       "spectral_bandwidth_min    0.000329\n",
       "spectral_centroid_max     0.000329\n",
       "spectral_centroid_min     0.000329\n",
       "rms_max                   0.000329\n",
       "rms_min                   0.000329\n",
       "mfcc_max                  0.000329\n",
       "melspectogram_max         0.000329\n",
       "mel_to_stft_min           0.000329\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualise the top 10 features that caused the major drop\n",
    "# in the roc-auc (aka model performance)\n",
    "\n",
    "feature_importance.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original number of features (rows in this case)\n",
    "feature_importance.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of features that cause a drop in performance\n",
    "# when shuffled\n",
    "\n",
    "feature_importance[feature_importance>0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 30 out of the 108 features caused a drop in the performance of the random forests when their values were permuted. This means that we could select those features and discard the rest, and should keep the original random forest performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chroma_stft_min', 'chroma_stft_max', 'chroma_cqt_min',\n",
       "       'chroma_cqt_max', 'chroma_cens_min', 'chroma_cens_max',\n",
       "       'melspectogram_min', 'melspectogram_max', 'mfcc_min', 'mfcc_max',\n",
       "       'rms_min', 'rms_max', 'spectral_centroid_min', 'spectral_centroid_max',\n",
       "       'spectral_bandwidth_min', 'spectral_bandwidth_max',\n",
       "       'spectral_contrast_min', 'spectral_contrast_max',\n",
       "       'spectral_flatness_min', 'spectral_flatness_max',\n",
       "       'spectral_rolloff_min', 'spectral_rolloff_max', 'poly_features_min',\n",
       "       'poly_features_max', 'tonnetz_min', 'tonnetz_max',\n",
       "       'zero_crossing_rate_min', 'zero_crossing_rate_max', 'tempogram_min',\n",
       "       'tempogram_max', 'delta_mfcc_min', 'delta_mfcc_max', 'mel_to_stft_min',\n",
       "       'mel_to_stft_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the important features\n",
    "\n",
    "feature_importance[feature_importance>0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chroma_stft_min', 'chroma_stft_max', 'chroma_cqt_min',\n",
       "       'chroma_cqt_max', 'chroma_cens_min', 'chroma_cens_max',\n",
       "       'melspectogram_min', 'melspectogram_max', 'mfcc_min', 'mfcc_max',\n",
       "       'rms_min', 'rms_max', 'spectral_centroid_min', 'spectral_centroid_max',\n",
       "       'spectral_bandwidth_min', 'spectral_bandwidth_max',\n",
       "       'spectral_contrast_min', 'spectral_contrast_max',\n",
       "       'spectral_flatness_min', 'spectral_flatness_max',\n",
       "       'spectral_rolloff_min', 'spectral_rolloff_max', 'poly_features_min',\n",
       "       'poly_features_max', 'tonnetz_min', 'tonnetz_max',\n",
       "       'zero_crossing_rate_min', 'zero_crossing_rate_max', 'tempogram_min',\n",
       "       'tempogram_max', 'delta_mfcc_min', 'delta_mfcc_max', 'mel_to_stft_min',\n",
       "       'mel_to_stft_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's build a random forests only with the selected features\n",
    "\n",
    "# capture the selected features\n",
    "selected_features = feature_importance[feature_importance > 0].index\n",
    "\n",
    "# train a new random forests using only the selected features\n",
    "rf = RandomForestClassifier(n_estimators=50,\n",
    "                            max_depth=2,\n",
    "                            random_state=2909,\n",
    "                            n_jobs=4)\n",
    "\n",
    "rf.fit(X_train[selected_features], y_train)\n",
    "\n",
    "# print roc-auc in train and testing sets\n",
    "\n",
    "print('train auc score: ',roc_auc_score(y_train, (rf.predict_proba(X_train[selected_features])),multi_class=\"ovr\"))\n",
    "print('test auc score: ',roc_auc_score(y_test, (rf.predict_proba(X_test[selected_features])),multi_class=\"ovr\"))\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Boost roc-auc: 0.8374698536641564\n",
      "GradientBoostingClassifier roc-auc: 0.9969756715990477\n",
      "HistGradientBoostingClassifier roc-auc: 1.0\n",
      "ExtraTreesClassifier roc-auc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier roc-auc: 0.959613804805104\n",
      "Decision Tree Accuracy: 0.9976198825013437\n",
      "BaggingClassifier roc-auc: 0.7220450330750114\n",
      "KNN {}nn score: {} 0.888264048750608\n",
      "Accuracy of Naive Bayes Algo:  0.7841478742298282\n",
      "Accuracy of NeighborhoodComponentsAnalysis: 1.0\n",
      "Accuracy of MLPClassifier 0.743599156055825\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train[selected_features]\n",
    "X_test =  X_test[selected_features]\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict_proba(X_train)\n",
    "print('Ada Boost roc-auc: {}'.format(roc_auc_score(y_train, y_pred,multi_class=\"ovo\")))\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "  max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict_proba(X_train)\n",
    "print('GradientBoostingClassifier roc-auc: {}'.format(roc_auc_score(y_train, y_pred,multi_class=\"ovo\")))\n",
    "\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict_proba(X_train)\n",
    "print('HistGradientBoostingClassifier roc-auc: {}'.format(roc_auc_score(y_train, y_pred,multi_class=\"ovo\")))\n",
    "\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "     min_samples_split=2, random_state=0)\n",
    "\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict_proba(X_train)\n",
    "print('ExtraTreesClassifier roc-auc: {}'.format(roc_auc_score(y_train, y_pred,multi_class=\"ovo\")))\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(\n",
    "     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "     voting='soft')\n",
    "\n",
    "params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "grid = grid.fit(X_train,y_train)\n",
    "y_pred = grid.predict_proba(X_train)\n",
    "print('Voting Classifier roc-auc: {}'.format(roc_auc_score(y_train, y_pred,multi_class=\"ovo\")))\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=9)\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict_proba(X_train)\n",
    "print(\"Decision Tree Accuracy:\",roc_auc_score(y_train, y_pred,multi_class=\"ovo\"))\n",
    "\n",
    "clf = BaggingClassifier(base_estimator=SVC(),\n",
    "                        n_estimators=10, random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict_proba(X_train)\n",
    "print('BaggingClassifier roc-auc: {}'.format(roc_auc_score(y_train, y_pred,multi_class=\"ovo\")))\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors = 5)\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict_proba(X_train)\n",
    "\n",
    "print(\"KNN {}nn score: {}\",roc_auc_score(y_train, y_pred,multi_class=\"ovo\"))\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict_proba(X_train)\n",
    "print(\"Accuracy of Naive Bayes Algo: \", roc_auc_score(y_train, y_pred,multi_class=\"ovo\"))\n",
    "\n",
    "\n",
    "nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
    "n = []\n",
    "for i in range(500):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i+1)\n",
    "    clf = Pipeline([('nca', nca), ('knn', knn)])\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict_proba(X_train)\n",
    "    n.append(roc_auc_score(y_train, y_pred,multi_class=\"ovo\"))\n",
    "print(\"Accuracy of NeighborhoodComponentsAnalysis:\",max(n))\n",
    "\n",
    "\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=600).fit(X_train, y_train)\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict_proba(X_train)\n",
    "print(\"Accuracy of MLPClassifier\",roc_auc_score(y_train, y_pred,multi_class=\"ovo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the random forests with the selected features show a similar performance (or even slightly higher) to the random forests built using all of the features. And it provides a simpler, faster and more reliable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1004, 36)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('C:/Users/RAJENDRA REDDY/Downloads/finalData.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1004, 35)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((702, 34), (302, 34))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "feature_cols = ['chroma_stft_min', 'chroma_stft_max', 'chroma_cqt_min',\n",
    "       'chroma_cqt_max', 'chroma_cens_min', 'chroma_cens_max',\n",
    "       'melspectogram_min', 'melspectogram_max', 'mfcc_min', 'mfcc_max',\n",
    "       'rms_min', 'rms_max', 'spectral_centroid_min', 'spectral_centroid_max',\n",
    "       'spectral_bandwidth_min', 'spectral_bandwidth_max',\n",
    "       'spectral_contrast_min', 'spectral_contrast_max',\n",
    "       'spectral_flatness_min', 'spectral_flatness_max',\n",
    "       'spectral_rolloff_min', 'spectral_rolloff_max', 'poly_features_min',\n",
    "       'poly_features_max', 'tonnetz_min', 'tonnetz_max',\n",
    "       'zero_crossing_rate_min', 'zero_crossing_rate_max', 'tempogram_min',\n",
    "       'tempogram_max', 'delta_mfcc_min', 'delta_mfcc_max', 'mel_to_stft_min',\n",
    "       'mel_to_stft_max']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[feature_cols],data['class'],test_size=0.3,random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this method, it is necessary to reset the indeces of the returned \n",
    "# datasets\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ML algo with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse:  1.0768967419854518\n",
      "train r2:  0.4276288164482638\n",
      "\n",
      "test rmse:  1.0854408911392635\n",
      "test r2:  0.4003900240187429\n"
     ]
    }
   ],
   "source": [
    "# The first step to determine feature importance by feature shuffling\n",
    "# is to build the machine learning model for which we want to\n",
    "# select features\n",
    "\n",
    "# In this case, I will build Random Forests, but remember that\n",
    "# you can use this procedure for any other machine learning algorithm\n",
    "\n",
    "# I build few and shallow trees to avoid overfitting\n",
    "rf = RandomForestRegressor(n_estimators=100,\n",
    "                           max_depth=3,\n",
    "                           random_state=2909,\n",
    "                           n_jobs=4)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# print performance metrics\n",
    "print('train rmse: ', mean_squared_error(y_train, rf.predict(X_train), squared=False))\n",
    "print('train r2: ', r2_score(y_train, (rf.predict(X_train))))\n",
    "print()\n",
    "print('test rmse: ', mean_squared_error(y_test, rf.predict(X_test), squared=False))\n",
    "print('test r2: ', r2_score(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle features and asses performance drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell, I will shuffle one by one, each feature of the dataset\n",
    "# and then use the dataset with the shuffled variable to make predictions\n",
    "# using the random forests I trained in the previous cell\n",
    "\n",
    "# overall train rmse: using all the features\n",
    "train_rmse = mean_squared_error(y_train, rf.predict(X_train), squared=False)\n",
    "\n",
    "# list to capture the performance shift\n",
    "performance_shift = []\n",
    "\n",
    "# for each feature:\n",
    "for feature in X_train.columns:\n",
    "    \n",
    "    X_train_c = X_train.copy()\n",
    "\n",
    "    # shuffle individual feature\n",
    "    X_train_c[feature] = X_train_c[feature].sample(frac=1, random_state=11).reset_index(\n",
    "        drop=True)\n",
    "\n",
    "    # make prediction with shuffled feature and calculate roc-auc\n",
    "    shuff_rmse = mean_squared_error(y_train, rf.predict(X_train_c), squared=False)\n",
    "    \n",
    "    drift = train_rmse - shuff_rmse \n",
    "\n",
    "    # store the drop in roc-auc\n",
    "    performance_shift.append(drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chroma_stft_min   -0.000918\n",
       "chroma_stft_max    0.000000\n",
       "chroma_cqt_min    -0.005392\n",
       "chroma_cqt_max     0.000000\n",
       "chroma_cens_min   -0.001040\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now I will transform the list into a pandas Series\n",
    "# for easy manipulation\n",
    "\n",
    "feature_importance = pd.Series(performance_shift)\n",
    "\n",
    "# add variable names in the index\n",
    "feature_importance.index = X_train.columns\n",
    "\n",
    "feature_importance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note here that when looking at the rmse, the smaller the better.\n",
    "\n",
    "# as we do original_rmse - shuffled_data_rmse\n",
    "\n",
    "# if the feature was important, the shuffled data would increase the rsme\n",
    "\n",
    "# thus, we are looking for negative values here\n",
    "\n",
    "# number of features that cause a drop in performance\n",
    "# when shuffled\n",
    "\n",
    "feature_importance[feature_importance<0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chroma_stft_min', 'chroma_cqt_min', 'chroma_cens_min',\n",
       "       'chroma_cens_max', 'melspectogram_min', 'melspectogram_max', 'mfcc_min',\n",
       "       'mfcc_max', 'rms_min', 'rms_max', 'spectral_centroid_min',\n",
       "       'spectral_centroid_max', 'spectral_bandwidth_min',\n",
       "       'spectral_bandwidth_max', 'spectral_contrast_min',\n",
       "       'spectral_contrast_max', 'spectral_flatness_min',\n",
       "       'spectral_flatness_max', 'spectral_rolloff_min', 'spectral_rolloff_max',\n",
       "       'poly_features_min', 'poly_features_max', 'tonnetz_min', 'tonnetz_max',\n",
       "       'zero_crossing_rate_min', 'zero_crossing_rate_max', 'delta_mfcc_min',\n",
       "       'delta_mfcc_max', 'mel_to_stft_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the variable names\n",
    "\n",
    "feature_importance[feature_importance<0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compare the performance of a random forest\n",
    "# built only using the selected features\n",
    "\n",
    "# slice the data\n",
    "feat = feature_importance[feature_importance<0].index\n",
    "\n",
    "X_train = X_train[feat]\n",
    "X_test = X_test[feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((702, 29), (702, 29))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with less features shows similar performance to that with all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse:  1.0766863652680665\n",
      "train r2:  0.4278524252760294\n",
      "\n",
      "test rmse:  1.0875362655862453\n",
      "test r2:  0.3980727718508713\n"
     ]
    }
   ],
   "source": [
    "# build and evaluate the model\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=2000,\n",
    "                           max_depth=3,\n",
    "                           random_state=2909,\n",
    "                           n_jobs=4)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# print performance metrics\n",
    "print('train rmse: ', mean_squared_error(y_train, rf.predict(X_train), squared=False))\n",
    "print('train r2: ', r2_score(y_train, (rf.predict(X_train))))\n",
    "print()\n",
    "print('test rmse: ', mean_squared_error(y_test, rf.predict(X_test), squared=False))\n",
    "print('test r2: ', r2_score(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3980727718508714"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = rf.predict(X_test)\n",
    "rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.41721854304635764\n",
      "Regressor Accuracy: 0.40066225165562913\n",
      "KNN 5nn score: 0.37748344370860926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of svm algo:  0.4105960264900662\n",
      "Accuracy of Naive Bayes Algo:  0.4105960264900662\n",
      "Accuracy of Random Forest Algo:  0.4867549668874172\n",
      "Accuracy of NeighborhoodComponentsAnalysis: 0.4370860927152318\n",
      "Accuracy of GradientBoostingClassifier: 0.46357615894039733\n",
      "Accuracy of MLPClassifier 0.347682119205298\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import (NeighborhoodComponentsAnalysis,KNeighborsClassifier)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=9)\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "LogReg_clf = LogisticRegression(random_state = 1)\n",
    "LogReg_clf.fit(X_train, y_train)\n",
    "y_pred = LogReg_clf.predict(X_test)\n",
    "acc =  accuracy_score(y_test, y_pred)\n",
    "print(\"Regressor Accuracy:\",acc)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train,y_train)\n",
    "prediction = knn.predict(X_test)\n",
    "\n",
    "print(\"KNN {}nn score: {}\".format(5,knn.score(X_test,y_test)))\n",
    "\n",
    "\n",
    "svm = SVC(random_state = 1)\n",
    "svm.fit(X_train,y_train)\n",
    "print(\"Accuracy of svm algo: \",svm.score(X_test,y_test))\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "print(\"Accuracy of Naive Bayes Algo: \", nb.score(X_test,y_test))\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\n",
    "rf.fit(X_train,y_train)\n",
    "print(\"Accuracy of Random Forest Algo: \", rf.score(X_test,y_test))\n",
    "\n",
    "\n",
    "nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
    "n = []\n",
    "for i in range(500):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i+1)\n",
    "\n",
    "    nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\n",
    "    nca_pipe.fit(X_train, y_train)\n",
    "    n.append(nca_pipe.score(X_test, y_test))\n",
    "print(\"Accuracy of NeighborhoodComponentsAnalysis:\",max(n))\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "print(\"Accuracy of GradientBoostingClassifier:\",clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=600).fit(X_train, y_train)\n",
    "print(\"Accuracy of MLPClassifier\",clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all for this lecture, I hope you enjoyed it and see you in the next one!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
