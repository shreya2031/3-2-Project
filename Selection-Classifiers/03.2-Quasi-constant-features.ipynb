{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi-constant features\n",
    "\n",
    "Quasi-constant features are those that show the same value for the great majority of the observations of the dataset. In general, these features provide little, if any, information that allows a machine learning model to discriminate or predict a target. But there can be exceptions. So you should be careful when removing these type of features.\n",
    "\n",
    "Identifying and removing quasi-constant features, is an easy first step towards feature selection and more interpretable machine learning models.\n",
    "\n",
    "Here, I will demonstrate how to identify quasi-constant features using a dataset that I created for this course. \n",
    "\n",
    "To identify quasi-constant features, we can use the VarianceThreshold from Scikit-learn, or we can code it ourselves. If we use the VarianceThreshold, all our variables need to be numerical. If we code it manually however, we can apply the code to both numerical and categorical variables.\n",
    "\n",
    "I will show 2 snippets of code, 1 where I use the VarianceThreshold and 1 manually coded alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204, 36)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "# (feel free to write some code to explore the dataset and become\n",
    "# familiar with it ahead of this demo)\n",
    "\n",
    "data = pd.read_csv('C:/Users/RAJENDRA REDDY/Downloads/Genre0.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 34), (62, 34))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate dataset into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['class','song'], axis=1), # drop the target\n",
    "    data['class'], # just the target\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove constant features\n",
    "\n",
    "First, I will remove constant features like I did in the previous lecture. This will allow a better visualisation of the quasi-constant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 30), (62, 30))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the code from the previous lecture\n",
    "# I remove 34 constant features\n",
    "\n",
    "constant_features = [\n",
    "    feat for feat in X_train.columns if X_train[feat].std() == 0\n",
    "]\n",
    "\n",
    "X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove quasi-constant features\n",
    "\n",
    "### Using the VarianceThreshold from sklearn\n",
    "\n",
    "The VarianceThreshold from sklearn provides a simple baseline approach to feature selection. It removes all features which variance doesnâ€™t meet a certain threshold. By default, it removes all zero-variance features, as we did in the previous notebook.\n",
    "\n",
    "Here, we will change the default threshold to remove quasi-constant features, or, I should better say, features with low-variance:\n",
    "\n",
    "Check the Scikit-learn docs for more details:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VarianceThreshold(threshold=0.01)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = VarianceThreshold(threshold=0.01)  \n",
    "\n",
    "sel.fit(X_train)  # fit finds the features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['melspectogram_max', 'mfcc_min', 'mfcc_max', 'rms_max',\n",
       "       'spectral_centroid_min', 'spectral_centroid_max',\n",
       "       'spectral_bandwidth_min', 'spectral_bandwidth_max',\n",
       "       'spectral_contrast_min', 'spectral_contrast_max',\n",
       "       'spectral_rolloff_min', 'spectral_rolloff_max', 'poly_features_min',\n",
       "       'poly_features_max', 'zero_crossing_rate_max', 'delta_mfcc_min',\n",
       "       'delta_mfcc_max', 'mel_to_stft_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_support is a boolean vector that indicates which features \n",
    "# are retained, that is, which features have a higher variance than\n",
    "# the threshold we indicated.\n",
    "\n",
    "# If we sum over get_support, we get the number\n",
    "# of features that are not quasi-constant\n",
    "\n",
    "print(sum(sel.get_support()))\n",
    "not_quasi_constant = X_train.columns[sel.get_support()]\n",
    "not_quasi_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's print the number of quasi-constant features\n",
    "\n",
    "quasi_constant = X_train.columns[~sel.get_support()]\n",
    "\n",
    "len(quasi_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 51 columns / variables are almost constant. This means that 51 variables show predominantly one value for the majority of observations of the training set. Let's explore a few if these variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chroma_stft_min', 'chroma_cqt_min', 'chroma_cens_min',\n",
       "       'chroma_cens_max', 'melspectogram_min', 'rms_min', 'rms_max',\n",
       "       'spectral_flatness_min', 'spectral_flatness_max',\n",
       "       'zero_crossing_rate_min', 'tempogram_min'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's print the variable names\n",
    "quasi_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'var_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'var_1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d5ad38f371c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# of the variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'var_1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'var_1'"
     ]
    }
   ],
   "source": [
    "# percentage of observations showing each of the different values\n",
    "# of the variable\n",
    "\n",
    "X_train['var_1'].value_counts() / np.float(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that > 99% of the observations show one value, 0. Therefore, this features is fairly constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.999971\n",
       "1    0.000029\n",
       "Name: var_2, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's explore another one\n",
    "\n",
    "X_train['var_2'].value_counts() / np.float(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and explore the rest of the quasi-constant variables.\n",
    "\n",
    "We can then remove the quasi-constant features utilizing the transform() method from the VarianceThreshold. Remember that this returns a NumPy array without feature names, so if we want a dataframe we need to reconstitute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['melspectogram_max', 'mfcc_min', 'mfcc_max', 'spectral_centroid_min',\n",
       "       'spectral_centroid_max', 'spectral_bandwidth_min',\n",
       "       'spectral_bandwidth_max', 'spectral_contrast_min',\n",
       "       'spectral_contrast_max', 'spectral_rolloff_min', 'spectral_rolloff_max',\n",
       "       'poly_features_min', 'poly_features_max', 'tonnetz_min', 'tonnetz_max',\n",
       "       'zero_crossing_rate_max', 'delta_mfcc_min', 'delta_mfcc_max',\n",
       "       'mel_to_stft_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# capture feature names\n",
    "\n",
    "feat_names = X_train.columns[sel.get_support()]\n",
    "feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 18), (62, 18))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove the quasi-constant features\n",
    "\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One class SVM\n",
      "Accuracy Score: 0.979\n",
      "F1 Score: 0.989\n",
      "Precision Score: 0.979\n",
      "Recall Score: 0.979\n",
      "EllipticEnvelope\n",
      "Accuracy Score: 0.894\n",
      "F1 Score: 0.944\n",
      "Precision Score: 0.894\n",
      "Recall Score: 0.894\n",
      "Isolation forest\n",
      "Accuracy Score: 0.915\n",
      "F1 Score: 0.956\n",
      "Precision Score: 0.915\n",
      "Recall Score: 0.915\n",
      "LocalOutlierFactor\n",
      "Accuracy Score: 0.915\n",
      "F1 Score: 0.956\n",
      "Precision Score: 0.915\n",
      "Recall Score: 0.915\n"
     ]
    }
   ],
   "source": [
    "trainy, testy = y_train, y_test\n",
    "# define outlier detection model\n",
    "trainX = X_train\n",
    "testX =  X_test\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from numpy import vstack\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "model = OneClassSVM(gamma='scale', nu=0.02)\n",
    "model.fit(trainX)\n",
    "yhat = model.predict(trainX)\n",
    "# mark inliers 1, outliers -1\n",
    "print('One class SVM')\n",
    "print('Accuracy Score: %.3f' % accuracy_score(y_train, yhat))\n",
    "print('F1 Score: %.3f' % f1_score(y_train, yhat, pos_label=1))\n",
    "print('Precision Score: %.3f' % precision_score(y_train, yhat, average='micro'))\n",
    "print('Recall Score: %.3f' % recall_score(y_train, yhat, average='micro'))\n",
    "\n",
    "\n",
    "model = EllipticEnvelope(contamination=0.1)\n",
    "model.fit(trainX)\n",
    "yhat = model.predict(trainX)\n",
    "# mark inliers 1, outliers -1\n",
    "\n",
    "# calculate score\n",
    "print('EllipticEnvelope')\n",
    "print('Accuracy Score: %.3f' % accuracy_score(y_train, yhat))\n",
    "print('F1 Score: %.3f' % f1_score(y_train, yhat, pos_label=1))\n",
    "print('Precision Score: %.3f' % precision_score(y_train, yhat, average='micro'))\n",
    "print('Recall Score: %.3f' % recall_score(y_train, yhat, average='micro'))\n",
    "\n",
    "\n",
    "# make a prediction with a lof model\n",
    "def lof_predict(model, trainX, testX):\n",
    "\t# create one large dataset\n",
    "\tcomposite = vstack((trainX, testX))\n",
    "\t# make prediction on composite dataset\n",
    "\tyhat = model.fit_predict(composite)\n",
    "\t# return just the predictions on the test set\n",
    "\treturn yhat[len(trainX):]\n",
    "\n",
    "\n",
    "model = IsolationForest(contamination=0.1)\n",
    "model.fit(trainX)\n",
    "yhat = lof_predict(model,testX,trainX)\n",
    "# mark inliers 1, outliers -1\n",
    "\n",
    "print('Isolation forest')\n",
    "print('Accuracy Score: %.3f' % accuracy_score(y_train, yhat))\n",
    "print('F1 Score: %.3f' % f1_score(y_train, yhat, pos_label=1))\n",
    "print('Precision Score: %.3f' % precision_score(y_train, yhat, average='micro'))\n",
    "print('Recall Score: %.3f' % recall_score(y_train, yhat, average='micro'))\n",
    "\n",
    "model = LocalOutlierFactor(contamination=0.1)\n",
    "yhat = lof_predict(model,testX,trainX)\n",
    "# mark inliers 1, outliers -1\n",
    "print('LocalOutlierFactor')\n",
    "print('Accuracy Score: %.3f' % accuracy_score(y_train, yhat))\n",
    "print('F1 Score: %.3f' % f1_score(y_train, yhat, pos_label=1))\n",
    "print('Precision Score: %.3f' % precision_score(y_train, yhat, average='micro'))\n",
    "print('Recall Score: %.3f' % recall_score(y_train, yhat, average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.993\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "# generate dataset\n",
    "\n",
    "# define outlier detection model\n",
    "model = EllipticEnvelope(contamination=0.01)\n",
    "# fit on majority class\n",
    "\n",
    "model.fit(trainX)\n",
    "# detect outliers in the test set\n",
    "yhat = model.predict(trainX)\n",
    "# mark inliers 1, outliers -1\n",
    "\n",
    "# calculate score\n",
    "score = f1_score(y_train, yhat, pos_label=1)\n",
    "print('F1 Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.993\n"
     ]
    }
   ],
   "source": [
    "# isolation forest for imbalanced classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# generate dataset\n",
    "\n",
    "# split into train/test sets\n",
    "\n",
    "# define outlier detection model\n",
    "model = IsolationForest(contamination=0.01)\n",
    "# fit on majority class\n",
    "\n",
    "model.fit(trainX)\n",
    "# detect outliers in the test set\n",
    "yhat = model.predict(trainX)\n",
    "# mark inliers 1, outliers -1\n",
    "\n",
    "# calculate score\n",
    "score = f1_score(trainy, yhat, pos_label=1)\n",
    "print('F1 Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalOutlierFactor\n",
      "Accuracy Score: 0.986\n",
      "F1 Score: 0.993\n",
      "Precision Score: 0.986\n",
      "Recall Score: 0.986\n"
     ]
    }
   ],
   "source": [
    "from numpy import vstack\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# make a prediction with a lof model\n",
    "def lof_predict(model, trainX, testX):\n",
    "\t# create one large dataset\n",
    "\tcomposite = vstack((trainX, testX))\n",
    "\t# make prediction on composite dataset\n",
    "\tyhat = model.fit_predict(composite)\n",
    "\t# return just the predictions on the test set\n",
    "\treturn yhat[len(trainX):]\n",
    "\n",
    "# generate dataset\n",
    "\n",
    "# split into train/test sets\n",
    "\n",
    "# define outlier detection model\n",
    "model = LocalOutlierFactor(contamination=0.01)\n",
    "# get examples for just the majority class\n",
    "\n",
    "# detect outliers in the test set\n",
    "yhat = lof_predict(model,testX,trainX)\n",
    "# mark inliers 1, outliers -1\n",
    "\n",
    "# calculate score\n",
    "model = LocalOutlierFactor(contamination=0.01)\n",
    "yhat = lof_predict(model,testX,trainX)\n",
    "# mark inliers 1, outliers -1\n",
    "print('LocalOutlierFactor')\n",
    "print('Accuracy Score: %.3f' % accuracy_score(y_train, yhat))\n",
    "print('F1 Score: %.3f' % score)\n",
    "print('Precision Score: %.3f' % precision_score(y_train, yhat, average='micro'))\n",
    "print('Recall Score: %.3f' % recall_score(y_train, yhat, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing constant and almost constant features, we reduced the feature space from 300 to 215. This means, that 85 features were removed from this dataset. Almost a third!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>melspectogram_max</th>\n",
       "      <th>mfcc_min</th>\n",
       "      <th>mfcc_max</th>\n",
       "      <th>rms_max</th>\n",
       "      <th>spectral_centroid_min</th>\n",
       "      <th>spectral_centroid_max</th>\n",
       "      <th>spectral_bandwidth_min</th>\n",
       "      <th>spectral_bandwidth_max</th>\n",
       "      <th>spectral_contrast_min</th>\n",
       "      <th>spectral_contrast_max</th>\n",
       "      <th>spectral_rolloff_min</th>\n",
       "      <th>spectral_rolloff_max</th>\n",
       "      <th>poly_features_min</th>\n",
       "      <th>poly_features_max</th>\n",
       "      <th>tonnetz_min</th>\n",
       "      <th>tonnetz_max</th>\n",
       "      <th>zero_crossing_rate_max</th>\n",
       "      <th>delta_mfcc_min</th>\n",
       "      <th>delta_mfcc_max</th>\n",
       "      <th>mel_to_stft_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>552.49475</td>\n",
       "      <td>-289.08110</td>\n",
       "      <td>169.05515</td>\n",
       "      <td>0.193857</td>\n",
       "      <td>696.721192</td>\n",
       "      <td>6754.124528</td>\n",
       "      <td>1473.225231</td>\n",
       "      <td>3952.254479</td>\n",
       "      <td>4.725219</td>\n",
       "      <td>48.074677</td>\n",
       "      <td>796.728516</td>\n",
       "      <td>9905.273438</td>\n",
       "      <td>0.253625</td>\n",
       "      <td>2.660251</td>\n",
       "      <td>-0.596752</td>\n",
       "      <td>0.647851</td>\n",
       "      <td>0.536133</td>\n",
       "      <td>-20.846766</td>\n",
       "      <td>26.345085</td>\n",
       "      <td>9.931176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>685.15735</td>\n",
       "      <td>-397.39444</td>\n",
       "      <td>169.58191</td>\n",
       "      <td>0.178042</td>\n",
       "      <td>995.816453</td>\n",
       "      <td>4185.681040</td>\n",
       "      <td>1321.707466</td>\n",
       "      <td>2943.124273</td>\n",
       "      <td>3.065951</td>\n",
       "      <td>53.888713</td>\n",
       "      <td>2153.320313</td>\n",
       "      <td>6976.757813</td>\n",
       "      <td>0.042618</td>\n",
       "      <td>1.694155</td>\n",
       "      <td>-0.393235</td>\n",
       "      <td>0.496960</td>\n",
       "      <td>0.313965</td>\n",
       "      <td>-22.865694</td>\n",
       "      <td>30.942684</td>\n",
       "      <td>9.972648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>341.73334</td>\n",
       "      <td>-493.72210</td>\n",
       "      <td>197.31262</td>\n",
       "      <td>0.141993</td>\n",
       "      <td>506.885685</td>\n",
       "      <td>1884.365113</td>\n",
       "      <td>811.292370</td>\n",
       "      <td>2018.019387</td>\n",
       "      <td>5.099440</td>\n",
       "      <td>47.304562</td>\n",
       "      <td>839.794922</td>\n",
       "      <td>3542.211914</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.543955</td>\n",
       "      <td>-0.570008</td>\n",
       "      <td>0.465677</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>-12.827309</td>\n",
       "      <td>22.043661</td>\n",
       "      <td>8.632686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201.66867</td>\n",
       "      <td>-374.01636</td>\n",
       "      <td>112.71471</td>\n",
       "      <td>0.112370</td>\n",
       "      <td>1244.511529</td>\n",
       "      <td>3793.308356</td>\n",
       "      <td>1498.854478</td>\n",
       "      <td>3007.642280</td>\n",
       "      <td>5.281376</td>\n",
       "      <td>44.970377</td>\n",
       "      <td>3025.415039</td>\n",
       "      <td>7030.590820</td>\n",
       "      <td>0.068378</td>\n",
       "      <td>0.929680</td>\n",
       "      <td>-0.454395</td>\n",
       "      <td>0.529148</td>\n",
       "      <td>0.256836</td>\n",
       "      <td>-24.823927</td>\n",
       "      <td>28.989141</td>\n",
       "      <td>8.438001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1572.45200</td>\n",
       "      <td>-248.96866</td>\n",
       "      <td>160.06061</td>\n",
       "      <td>0.311368</td>\n",
       "      <td>1160.999467</td>\n",
       "      <td>5808.814918</td>\n",
       "      <td>1090.000846</td>\n",
       "      <td>3483.642221</td>\n",
       "      <td>1.667117</td>\n",
       "      <td>50.155734</td>\n",
       "      <td>1765.722656</td>\n",
       "      <td>8817.846680</td>\n",
       "      <td>0.226985</td>\n",
       "      <td>2.929869</td>\n",
       "      <td>-0.517440</td>\n",
       "      <td>0.534140</td>\n",
       "      <td>0.501953</td>\n",
       "      <td>-21.953081</td>\n",
       "      <td>26.134052</td>\n",
       "      <td>12.696788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   melspectogram_max   mfcc_min   mfcc_max   rms_max  spectral_centroid_min  \\\n",
       "0          552.49475 -289.08110  169.05515  0.193857             696.721192   \n",
       "1          685.15735 -397.39444  169.58191  0.178042             995.816453   \n",
       "2          341.73334 -493.72210  197.31262  0.141993             506.885685   \n",
       "3          201.66867 -374.01636  112.71471  0.112370            1244.511529   \n",
       "4         1572.45200 -248.96866  160.06061  0.311368            1160.999467   \n",
       "\n",
       "   spectral_centroid_max  spectral_bandwidth_min  spectral_bandwidth_max  \\\n",
       "0            6754.124528             1473.225231             3952.254479   \n",
       "1            4185.681040             1321.707466             2943.124273   \n",
       "2            1884.365113              811.292370             2018.019387   \n",
       "3            3793.308356             1498.854478             3007.642280   \n",
       "4            5808.814918             1090.000846             3483.642221   \n",
       "\n",
       "   spectral_contrast_min  spectral_contrast_max  spectral_rolloff_min  \\\n",
       "0               4.725219              48.074677            796.728516   \n",
       "1               3.065951              53.888713           2153.320313   \n",
       "2               5.099440              47.304562            839.794922   \n",
       "3               5.281376              44.970377           3025.415039   \n",
       "4               1.667117              50.155734           1765.722656   \n",
       "\n",
       "   spectral_rolloff_max  poly_features_min  poly_features_max  tonnetz_min  \\\n",
       "0           9905.273438           0.253625           2.660251    -0.596752   \n",
       "1           6976.757813           0.042618           1.694155    -0.393235   \n",
       "2           3542.211914           0.015032           0.543955    -0.570008   \n",
       "3           7030.590820           0.068378           0.929680    -0.454395   \n",
       "4           8817.846680           0.226985           2.929869    -0.517440   \n",
       "\n",
       "   tonnetz_max  zero_crossing_rate_max  delta_mfcc_min  delta_mfcc_max  \\\n",
       "0     0.647851                0.536133      -20.846766       26.345085   \n",
       "1     0.496960                0.313965      -22.865694       30.942684   \n",
       "2     0.465677                0.166016      -12.827309       22.043661   \n",
       "3     0.529148                0.256836      -24.823927       28.989141   \n",
       "4     0.534140                0.501953      -21.953081       26.134052   \n",
       "\n",
       "   mel_to_stft_max  \n",
       "0         9.931176  \n",
       "1         9.972648  \n",
       "2         8.632686  \n",
       "3         8.438001  \n",
       "4        12.696788  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trasnform the array into a dataframe\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=feat_names)\n",
    "X_test = pd.DataFrame(X_test, columns=feat_names)\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding it ourselves\n",
    "\n",
    "First, I will separate the dataset into train and test and remove the constant features again. Then, I will provide an alternative method to find out quasi-constant features.\n",
    "\n",
    "This method, as opposed to the VarianceThreshold, can be used for both **numerical and categorical** variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 30), (62, 30))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['class','song'], axis=1),\n",
    "    data['class'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "# remove constant features\n",
    "# using the code from the previous lecture\n",
    "\n",
    "constant_features = [\n",
    "    feat for feat in X_train.columns if X_train[feat].std() == 0\n",
    "]\n",
    "\n",
    "X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rajendra reddy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an empty list\n",
    "quasi_constant_feat = []\n",
    "\n",
    "# iterate over every feature\n",
    "for feature in X_train.columns:\n",
    "\n",
    "    # find the predominant value, that is the value that is shared\n",
    "    # by most observations\n",
    "    predominant = (X_train[feature].value_counts() / np.float(\n",
    "        len(X_train))).sort_values(ascending=False).values[0]\n",
    "\n",
    "    # evaluate the predominant feature: do more than 99% of the observations\n",
    "    # show 1 value?\n",
    "    if predominant > 0.998:\n",
    "        \n",
    "        # if yes, add the variable to the list\n",
    "        quasi_constant_feat.append(feature)\n",
    "\n",
    "len(quasi_constant_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method was a bit more aggressive than VarianceThreshold from sklearn with the threshold that we selected above. It found 108 features that show predominantly 1 value for the majority of the observations. \n",
    "\n",
    "Let's see how some of the quasi constant features look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the feature names\n",
    "\n",
    "quasi_constant_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'var_3'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select one feature from the list\n",
    "\n",
    "quasi_constant_feat[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0000         0.999629\n",
       "35685.9459     0.000029\n",
       "3583.3941      0.000029\n",
       "15028.0560     0.000029\n",
       "52105.7901     0.000029\n",
       "10281.6000     0.000029\n",
       "86718.0000     0.000029\n",
       "207901.3365    0.000029\n",
       "25905.4866     0.000029\n",
       "5209.9500      0.000029\n",
       "2641.0164      0.000029\n",
       "12542.3100     0.000029\n",
       "861.0900       0.000029\n",
       "27.3000        0.000029\n",
       "Name: var_3, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['var_3'].value_counts() / np.float(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature shows 0 for more than 99.9% of the observations. But, it also shows a few different values for a very tiny proportion of the observations. This fact, will increase the feature variance, that is why, this feature is not captured by the VarianceThreshold in our previous cell. Yet, we can see that it is quasi-constant.\n",
    "\n",
    "Keep in mind that the thresholds are arbitrary and decided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 30), (62, 30))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally, let's drop the quasi-constant features:\n",
    "\n",
    "X_train.drop(labels=quasi_constant_feat, axis=1, inplace=True)\n",
    "X_test.drop(labels=quasi_constant_feat, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, how, we removed almost half of the original variables!!! We passed from 300 variables to 158."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all for this lecture, I hope you enjoyed it and see you in the next one!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
